---
layout: post
title:  Il meccanicismo probabilistico di Salmon
date:   2021-10-06 12:40:16
tags: filosofia
description: Introduzione alla teoria della spiegazione scientifica in Salmon. In questo articolo introduciamo il modello causale di Salmon che si inserisce nel filone del meccanicismo probabilistico

---

Ricordiamo che  ci sono approcci alla spiegazione che non sono di carattere causale e ci sono approcci alla causalità che non sono legati alla spiegazione sceintifica. Ci sono approcci invece che sono tutte e due. La proposta di Salmon si inserisce in quest'ultimo contesto e prende il nome di **meccanicismo probabilistico**. Cosa significa? Meccanicismo è termine che proviene dalla fisica newtoniana: dal determinismo di Laplace, Salmon recupera il concetto di causa entrato in crisi con la fisica contemporanea: la causa sono i fattori che contribusicono al produrre di qualcos'altro. Le cause agiscono in termini probabilistici mutando la probabilità che qualcos'altro di causato accada. Il termine meccanicismo si intende nel senso che la causalità viene concepita come produzione, una causa è un tipo di evento che produce, che porta ad essere. La causa è un rapporto di produttività, di produzione. Ma ci sono degli approcci che concepiscono la causalià in termini diversi da quelli produttivi. Sembra piuttosto distante dal modello di rilevanza statistica. Oggi vediamo come si passa da quest'ultimo al meccanicismo probabilistico.

Ricordiamo il limite dell approcio SR: questo lascia aperta la possibilità di rapporti di rilevanza statistica che non sono in relazione con rapporti di rilevanza esplicativa. Un primo stratagemma, riprendendolo da Reichenbach, è usare la regola di adombramento. Se siamo in grado di mettere in ombra in senso statistico allora possiamo scartare le prorietà adombrate. Le spiegazioni sintomatiche sono spiegazioni che presentano fattori statisticamente rilevanti e che sembra che spiegano il legame tra $$A$$ e $$B$$. Noi però siamo alla ricerca di spiegazioni genuine, non solo di quelle sintomatica: dobbiamo trovare modo di distinguere tra ciò che è sintomatico e tra ciò che spiega. La regola di adombramento è un primo passaggio dal livello statistico a quello causale: per Salmon la spiegazione che è una buona spiegazione consiste in due livelli: il primo, il livello statistico, sul quale si innesta il secondo, causale (anche se Salmon non presenta esempi concreti dell'intero processo esplicativo formulato su questi due livelli). Allora la spiegazione casuale fornisce una classe di spiegazione più oggettivamente omogenea  di una spiegazione statistica. Questo perché tipicamente le proprietà adombranti sono proprietà casuali. Inoltre possiamo distinguere tra pseudo spiegazioni (quelle sintomatiche) e tra spiegazioni bassate su correlazioni genuine. L'obiettivo per Salmon, ora, è cercare di indivudere le reti di spiegazioni casuali sottese ai fenomeni. Spiegare un evento significa narrare quali fili causali legano questo evento ad altri eventi, ovvero le sue cause. Molte relaziani causali esibisicono relazioni di adombramento, ma questo non è un criterio sufficiente per dire che c'è un legame di causalità. Allora Salmon ricorre al **principio di causa comune**, elaborato in precedenza da Reichenbach. Questo viene introdotto per spiegare in modo causale coincidenze apparantemente accidentali. Il principio dice che se due o più eventi in luoghi diversi accadono congiuntamente con frequenza maggiore di quanto non ci si aspetterebbe se fossero indipendenti, allora questa apparente coincidenza va spiegata nei termini di un comune antecedente causale. È un principio molto forte, ma vedremo che neppure questo sarà sufficiente per spiegazioni causali. Viene assunto per passare da un livello di spiegazione basato sulla dipendenza statistica a un livello basato sulla dipendenza causale. Andare a cercare la causa comune ci sposta su un altro livello. La causa comune rende quei due eventi indipendenti, sposta il rapporto esplicativo dal posto in cui non dovrebbe essere ovvero a livello statistico al posto in cui dovrebbe essere ovvero a livello esplicativo. 

Immaginiamo che $$A$$ e $$B$$ accadono congiuntamente più frequentemente di quanto non accadrebbe se fossero indipendenti, ovvero $$P(A \cdot B) > P(A)P(B)$$, e supponiamo che $$C$$ sia la causa. Ma se i due eventi fossero indipendenti varrebbe $P(A \cdot B) = P(A) \times P(B)$. Se inserisco un'altra variabile $$C$$ e la dipendenza statistica diventa indipendenza statistica ovvero $$ P( A \cdot B \vert C) = P(A \vert C) \times P(B \vert C)$$.  In presenza della causa comune $$C$$ l'dipendenza statistica $$P(A\cdot B) > P(A)P(B)$$ diventa indipendenza statistica $$P(A \cdot B\vert C) = P(A \vert C) \times P(B\vert C)$$, ovvero $$A$$ e $$B$$ si mostrano come due eventi indipendenti. Stessa cosa accade se al posto di $$C$$ mettiamo la sua negazione $$\tilde{C}$$[^1] . Inoltre vale anche che $$P(A\vert C) > P(A\vert\tilde{C})$$ e $$P(B\vert C)>P(B \vert \tilde{C})$$ perché in realtà sono due eventi indipendenti. Se la probabilità che io stia male è $$A$$ e la probabilità che l'altro stia male è $$B$$, allora vale  $$P(A\vert C) = P(A\vert B \cdot C)$$ (e vale anche simmetricamente $$P(B\vert C) = P(B\vert A \cdot C$$), ovvero $$B$$ diventa statisticamente irrilevante rispetto ad $$A$$ nel senso che il fatto che egli stia male è irrilevante rispetto al fatto che io stia male: la causa comune trasforma una dipendenza statistica in un'indipendenza. Vediamo come nella __biforcazione congiuntiva__ $$C$$ adombra $$A$$ da $$B$$ e $$B$$ da $$A$$. Notiamo una cosa: stiamo cercando di passare da un livello statistico a un livello causale usando ancora strumenti statistici e Salmon non ha ancora specificato cosa sia una causa. Infatti stiamo ancora usando enunciati nomici, l'unica cosa che facciamo è dare una interpetazione diversa alla variabile $$C$$, ma il punto è che stiamo cercando di dare rapporti causali tramite uguaglianze o disuguaglianze ancora di natura statistica. Questa è una prima obiezione. Possiamo farne una seconda:  come andiamo a cercare le cause? Nei casi espressi sopra come esempi il modello funziona perché è molto semplice. Questo tipo di strutture, ovvero la causa comune che spiega i due/tre effeti viene chiamata biforcazione congiuntiva. Ci sono degli esempi che cerca di fare Salmon: due o più persone che stanno male dopo cena, daltonismo tra fratelli. Le biforcazioni congiuntive esprimono una asimmetria che riflette la direzione temporale e il carattere asimimetrico della spiegazione: infatti si spiegano gli effetti riportandoli a cause ma non si fa il processo inverso [^2]. Da questa modellizzazione della biforcazione congiuntiva se $$C$$ causa $$E$$ vuol dire anche che $$C$$ precede $$E$$. Allora le cause vanno cercate nel cono di luce passato dell'evento che voglio spiegare. Pertanto la causa comune rende conto dell'indipendenza statistica e tra i due eventi discendenti dalla stessa causa $$C$$ non esiste una dipendenza causale diretta. Solo la causa comune spiega il verificarsi dei due eventi. Questa può essere usate per stabilire l'asimmetria temporale propria dei fenomeni causali dalla diagramma della biforcazione congiuntiva. Queste relazioni ci spostano da un tipo di spiegazione statistica a una spiegazione ancora espressa in termini statistici che però dà più rilevanza/importanza a certi particolari eventi a cui ci riferiamo come cause. Che cosa però rende $$C$$ una causa? Che cosa lo rende importante? In questo contesto per ora $$C$$ è una cosa soltanto per la posizione che assume in un certo tipo di rapporti statistici. Vedremo che non è ancora sufficiente. Notiamo bene una cosa: se $$E$$ è un effetto comune di $$A$$ e $$B$$, questo non mi spiega il verificarsi dei due eventi, ovvero $$P(A \cdot B| C \cdot E) = P(A \cdot B | C)$$, che non vuol dire altro che $$E$$ è edombrata da $$C$$. 








[^1]: Mi vien da chiedere allora chi individuiamo come causa, $$C$$ o la sua negazione $$\tilde{C}$$?
[^2]: C'è da tenere conto però che non tutti i fenomeni causali sono unidirezionali



